\section{Lessons learned perspective}
%Kaare vil gerne skrive om den 
% Taken from: https://github.com/itu-devops/lecture_notes/blob/39aaac80478c8f90870269760864ff32c23b5cce/REPORT.md
% OBS: THIS IS THE REPORT TEMPLATE FROM LAST YEAR

% Describe the biggest issues, how you solved them, and which are major lessons learned with regards to:

%     evolution and refactoring
%     operation, and
%     maintenance

% of your ITU-MiniTwit systems. Link back to respective commit messages, issues, tickets, etc. to illustrate these.

% Also reflect and describe what was the "DevOps" style of your work. For example, what did you do differently to previous development projects and how did it work?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ideas. Database problem/migration (minitwit crashes, 0 in all follower tables). Hack(s) introduction. Latency problem (database problem). DigitalOcean Memory problems. CI/CD change. Cookies not working in production. Password & API key leaking.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Throughout the project, the group was faced with many difficulties requiring careful considerations, due to our simulated users, needing uninterrupted access to the application.
\subsection{Migration of our database}
Our largest DevOps challenge throughout the project, was the necessary migration of our database from SQLite to Postgres %The problem originated early after the simulator was started. %A crash was noticed (see issue \href{https://github.com/DevelOpsITU/MiniTwit/issues/83}{\#83}), seemingly caused by a large tweet, 8000+ characters. Its discovery was through gin's automatic logging, visible in the docker logs found via ssh, since no logging nor monitoring existed at the time. The crash was repetitive. 
Our docker container repeatedly crashed due to a memory leak caused by SQLite, sometimes causing everything to disappear, including the current SQLite database file. \\

%In a subsequent issue \href{https://github.com/DevelOpsITU/MiniTwit/issues/86}{\#86}, it was discovered through local bug reproduction, that SQLite leaked memory, due to too many open connections. 
The group talked from the beginning of the project about separating the database and introducing a database abstraction layer (see issue \href{https://github.com/DevelOpsITU/MiniTwit/issues/37}{\#37}). However, had not done so, before the simulator had started. %, and due to the crashes it seemed to be time for the migration. A few SQL queries had to be changed, in particular ones querying the \textbf{user} table, because PostGRE has its own user table more prominent than ours. 
Very concerning was the discovery, that all passwords could not be brought over due to them being binary data (BLOB), which has no direct translation into Postgres' types, also discussed in the issue regarding the migration (see issue \href{https://github.com/DevelOpsITU/MiniTwit/issues/92}{\#92}). \\

%After testing everything locally, and without complications, we had the belief everything would go smoothly with production, however luckily we realised we had not migrated any mysterious password hashes in our local test, since the mysterious ones only were made in production (see issue \href{https://github.com/DevelOpsITU/MiniTwit/issues/109}{\#109}). 
The group found no way off translating the BLOB data, and decided to replace all passwords with the same working hash, so the group could move all of the database data. In a real life scenario the group would have decided to ask all users to change their password, which would result in all newly updated passwords to be created and stored compatible with the new database scheme.

%The migration was a success, however, we noticed through the now implemented simple logging, that a large amount of errors were returned to the simulator (see issue \href{https://github.com/DevelOpsITU/MiniTwit/issues/112}{\#112}. Worse was when we discovered looking into the follower table that multiple entries were nothing but zeroes. It seemed like no follow request returned OK. Resources was put into discovering the origin, which conclusively became that the simulator only used a small batch of users for following and un-following, who's registrations we had missed during our downtime and deletions of our database from crashes. The solution became introducing "\textit{a dirty hack}", that would register users that tried to follow. Our excuse for this smelly code, was that this would never happen in a real world scenario.\todo{reference to issues}

\subsubsection{Learnings}
From the group's unfortunate migration problems, the group learned to never put any database into production before having setup an ORM framework to help make sure a transition between database systems is easy. In a casual project the group would have deleted everything and started over in a fresh new database system with fresh empty tables. However, due to the DevOps part of the course, it became important to the group, to successfully move all the data, and try to have as little downtime as possible. \\

For the migration the group used the tool PGLoader\cite{pgloader}, although in future attempts the group may try something different, e.g. many database systems can save and load database tables as .csv files. This was an idea given by another group later in the course.

\subsection{Request latency}
Towards the end of the project, the group noticed an unusual long request latency for the front page of the application. It was concluded that the bottleneck was the database (see issue \href{https://github.com/DevelOpsITU/MiniTwit/issues/182}{\#182}). Either an odd query or the lack of an index could cause a slow lookup, causing the request latency.\\
For future projects, especially ones with a lot of data. More effort will be put into queries and tables in the database, to make sure it scales.

\subsection{Memory devouring monitoring server}
 At the end of the project another maintenance issue was observed. A constant crashing monitoring server. The monitoring server crashes constantly presumably due to missing memory in the container.  %Through the Digital Ocean's monitoring, it was visible, that 100 percent of the memory was in use when querying Prometheus from Grafana (see issue \href{https://github.com/DevelOpsITU/MiniTwit/issues/183}{\#183})}, causing the service to become unusable. 
In future projects, the group would ensure monitoring/logging services are provided more memory, due to a lot of data being held in memory when querying for these services. Additionally, the group did not have much filtering of logging and no set schedule and plan of storing/rotating the logging information, which led to the memory continuously increasing. 

% \subsection{Early logging}
% Logging should be introduced earlier so we easilier could've found errors

% \subsection{Infrstructure as code}
% Should have used Terraforming from the start instead of Vagrant

% \subsection{No frontend, no fun}
% A seperation of the frontend and backend was missing in our project, but something we came to miss... nah prob not

% \subsection{Nothing is free}
% Seperating everything into different services is easy to maintain and scale, but is expensive to run. Is the techincal debt of not doing it great enough?

% \subsection{Cannot Interrupt (CI)}
% Our CI pipeline never deploys/releases, we do not trust it.

